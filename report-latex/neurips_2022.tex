\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
%\usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint, nonatbib]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{biblatex}
\addbibresource{neurips_2022.bib}
\usepackage{pythonhighlight}

\title{Computer Vision Proseminar Report WS2023}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Markus Diller\\
    University of Salzburg\\
    Salzburg, 5020 \\
    \texttt{markus.diller@stud.plus.ac.at} \\
    \And
    Marcel Sargant \\
    University of Salzburg \\
    Salzburg, 5020 \\
    \texttt{marcel.sargant@stud.plus.ac.at} \\
}


\begin{document}


    \maketitle


    \begin{abstract}
        The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on both the left- and right-hand margins.
        Use 10~point type, with a vertical spacing (leading) of 11~points.
        The word \textbf{Abstract} must be centered, bold, and in point size 12.
        Two line spaces precede the abstract.
        The abstract must be limited to one paragraph.
    \end{abstract}


    \section{Introduction}\label{sec:introduction}


    \section{ConvNeXt}\label{sec:convnext}
    ConvNeXt\cite{liu2022convnet}.


    \section{Documentation}\label{sec:documentation}
    For our work with the ConvNeXt architecture we investigated three main ideas.
    The first and most thorough was the effect of data augmentation on the network performance.
    The second was to experiment with the finetuning provided by the given implementation.
    Finally, the third focus was to revert some of the changes~\Citeauthor{liu2022convnet} made to the ResNet they built upon.

    As it is always a difficult choice to pick the hyperparameters, in our experiments we just used two different configurations that henceforth will be called \texttt{paper} and \texttt{example}, respectively.
    The first set was taken from the second column in Table 5 of the ConvNeXt\cite{liu2022convnet} paper and the second were taken from the example Colab project presented on the networks GitHub page\cite{ayush0finetune}.
    \autoref{tab:hyperparameters} shows all hyperparameters that are not set to the default values.
    What additionally differs from the setup in the paper is the dataset used.
    Instead of Imagenet1k all our training runs are based on the CIFAR10\cite{krizhevsky2009learning} dataset reduced to 10\% of its original size.
    \begin{table}[h]
        \caption{Choice of hyperparameters.}
        \begin{center}
            \begin{tabular}{lll}
                \hline
                Parameter     & \texttt{paper} & \texttt{example} \\ \hline
                learning rate & 4e-3           & 4e-4             \\
                warmup epochs & 5              & 0                \\
                cutmix        & 1              & 0                \\
                mixup         & 0.8            & 0                \\
                EMA decay     & 1              & 0.9999
            \end{tabular}
        \end{center}
        \label{tab:hyperparameters}
    \end{table}

    \subsection{Code Modification and Quality of Life}\label{subsec:code-modification}
    To allow for an improved workflow for testing we chose to make some minor changes to the given codebase.
    This was done to not only ensure faster training times, but also for a more lenient experience when having to set up the environment.

    The first and only change that meddles with the actual implementation was to add another command line argument to allow for downsampling CIFAR10.
    For this we registered a \texttt{--downsample} argument to the used argparser that is then used in the \verb|build_dataset| function in the \texttt{dataset.py} file.
    The implementation for the usage can be seen in~\ref{fig:downsampling}.
    Instead of choosing a random subset we choose every $n$th element, where $n$ is the downsampling factor.
    Additionally, for transparency, the original codebase used CIFAR100, but we chose to change it to CIFAR10.
    \begin{figure}[h]
        \begin{python}
            dataset = datasets.CIFAR10(args.data_path, train=is_train,
            transform=transform, download=True)
            sample = list(range(0, len(dataset), args.downsample))
            dataset = torch.utils.data.Subset(dataset, sample)
            nb_classes = 10
        \end{python}
        \caption{Downsampling the dataset.}
        \label{fig:downsampling}


    \end{figure}

    The second and final change was to create a \texttt{requirements.txt} file for easy installation of the necessary packages.
    Due to the codebase being incompatible with versions of pytorch larger than 2.0 and Google Colab defaulting to such a version we created this file to easily install all necessary dependencies with a single command.
    This comes especially in handy because Colab resets the runtime after just a few hours of inactivity, in turn also resetting the changes in installed packages.
    It eludes us as to why there was no such file already included in the codebase, just a very bare-bones markdown file with requirements.

    \subsection{Data Augmentation}\label{subsec:data-augmentation}

    \subsection{Finetuning}\label{subsec:finetuning}

    \subsection{Network Reversion}\label{subsec:network-reversion}


    \section{Conclusion}\label{sec:conclusion}

    \printbibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section*{Checklist}
    \input{checklist}

\end{document}